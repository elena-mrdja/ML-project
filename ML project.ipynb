{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CSE204 - Project : Relationship prediction through SMS chat analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gde sam ovo nasla: https://medium.com/analytics-vidhya/text-based-communication-analysis-with-machine-learning-17138c0a4a4e\n",
    "nltk tutorijal: https://www.digitalocean.com/community/tutorials/how-to-perform-sentiment-analysis-in-python-3-using-the-natural-language-toolkit-nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ovo nam moze pomoci ako hocemo da analiziramo vreme slanja i ucestalost poruka\n",
    "df_text=pd.read_csv('text_sample.csv')\n",
    "df_text.info()\n",
    "#treba da vidimo da odlucimo koje kolone hocemo za nase podatke, ovo je primer: \n",
    "\"\"\"<class 'pandas.core.frame.DataFrame'>\n",
    "RangeIndex: 62513 entries, 0 to 62512\n",
    "Data columns (total 7 columns):\n",
    "message_type    62513 non-null object\n",
    "text            62513 non-null object\n",
    "time            62513 non-null object\n",
    "ID              62513 non-null object\n",
    "group_list      62513 non-null object\n",
    "term            62513 non-null object\n",
    "sent_by         62513 non-null object\n",
    "dtypes: object(7)\n",
    "memory usage: 3.3+ MB\"\"\"\n",
    "\n",
    "#Extracting year and month from 'time' column and append the dataframe with new columns.\n",
    "df_text['year'] = pd.DatetimeIndex(df_text['time']).year\n",
    "df_text['month'] = pd.DatetimeIndex(df_text['time']).month\n",
    "df_text.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ovako se broji ko je poslao koliko poruka mesecno, moze da bude korisno za jednostrana muvanja :) \n",
    "\n",
    "# Getting the message count by message type and month\n",
    "# reset_index() gives a column for counting, after groupby uses year and category\n",
    "df_cnt = (df_text.reset_index()\n",
    "          .groupby(['month','message_type'], as_index=False)\n",
    "          .count()\n",
    "          # rename isn't strictly necessary here, it's just for readability\n",
    "          .rename(columns={'index':'count'})\n",
    "       )\n",
    "#sorting the values by month\n",
    "df_cnt.sort_values(by = 'month', ascending = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ciscenje podataka od nebitnih stvari pomocu ovog nlkta, ovo treba proveriti da li nam stvarno treba\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "set(stopwords.words('english'))\n",
    "from nltk.tokenize import word_tokenize\n",
    "from contractions import contractions_dict\n",
    "import unicodedata\n",
    "\n",
    "def remove_punctuation(text):\n",
    "    text = ''.join([i for i in text if not i.isdigit()])\n",
    "    return re.sub(\"[!@#$+%*:()/|,;:'-]\", ' ', text)\n",
    "\n",
    "def removebrackets(text):\n",
    "    return re.sub('[\\(\\[].*?[\\)\\]]', ' ', text)\n",
    "\n",
    "def remove_accented_chars(text):\n",
    "    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')\n",
    "\n",
    "def remove_special_chars(text, remove_digits=False):\n",
    "    pattern = r'[^a-zA-Z0-9\\s]' if not remove_digits else r'[^a-zA-Z\\s]'\n",
    "    return re.sub(pattern, '', text)\n",
    "\n",
    "def remove_stopwords(text):\n",
    "   # text_tokens = word_tokenize(text)\n",
    "   # text = remove_stopwords(text)\n",
    "      \n",
    "    from gensim.parsing.preprocessing import STOPWORDS\n",
    "    all_stopwords_gensim = STOPWORDS.union(set(['thank', 'need', 'yes', 'okay']))\n",
    "    text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = ' '.join([word for word in text_tokens if not word in all_stopwords_gensim])\n",
    "    \n",
    "    return tokens_without_sw\n",
    "\n",
    "def stemming (text):\n",
    "    \n",
    "    ps = nltk.porter.PorterStemmer()\n",
    "    return ' '.join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "    stopword_list = stopwords.words('english')\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "    return ' '.join([token for token in tokens if token not in stopword_list])\n",
    "\n",
    "def lemmatize(text):\n",
    "    text = nlp(text)\n",
    "    return ' '.join([word.lemma_ if word.lemma_ != '-PRON-' else word.text for word in text])\n",
    "\n",
    "def expand_contractions(text, contraction_mapping=contractions_dict):\n",
    "    \n",
    "    contractions_pattern = re.compile('({})'.format('|'.join(contraction_mapping.keys())), \n",
    "                                      flags=re.IGNORECASE|re.DOTALL)\n",
    "def expand_match(contraction):\n",
    "    match = contraction.group(0)\n",
    "    first_char = match[0]\n",
    "    expanded_contraction = contraction_mapping.get(match)\\\n",
    "                            if contraction_mapping.get(match)\\\n",
    "                            else contraction_mapping.get(match.lower())                       \n",
    "    expanded_contraction = first_char+expanded_contraction[1:]\n",
    "    return expanded_contraction\n",
    "\n",
    "    expanded_text = contractions_pattern.sub(expand_match, text)\n",
    "    return re.sub(\"'\", \"\", expanded_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ovo pravi worldcloud, nacin da se lepo predstave najucestalije reci, nzm da li nam stvarno treba ali neka ga ovde\n",
    "\n",
    "from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n",
    "wc = WordCloud(stopwords=STOPWORDS,max_font_size=200, max_words=1000000, background_color=\"white\", width=1000, height=1000).generate(' '.join(df_filtered['text_clean']))\n",
    "plt.figure(figsize=(20,20))\n",
    "plt.imshow(wc)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ovo nam sluzi da izaberemo jos reci koje hocemo da izbacimo \n",
    "\n",
    "def clean_text_again(text):\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "all_stopwords_gensim = STOPWORDS.union(set(['thank', 'need', 'yes', 'okay','ok','thanks','morning','hello','sure','hi', 'know', 'got','yesterday']))\n",
    "text_tokens = word_tokenize(text)\n",
    "    tokens_without_sw = [word for word in text_tokens if not word in all_stopwords_gensim]\n",
    "return ' '.join([word for word in text_tokens if not word in all_stopwords_gensim])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ovo je korisno, sluzi da prepoznamo sintagme reci koje se cesto pojavljuju zajedno\n",
    "\n",
    "def plot_top_ngrams_barchart(text, n=3): #n is the number of immediate words you need.\n",
    "      \n",
    "    new= text.str.split()\n",
    "    new=new.values.tolist()\n",
    "    corpus=[word for i in new for word in i]\n",
    "def _get_top_ngram(corpus, n=None):\n",
    "        vec = CountVectorizer(ngram_range=(n, n)).fit(corpus)\n",
    "        bag_of_words = vec.transform(corpus)\n",
    "        sum_words = bag_of_words.sum(axis=0) \n",
    "        words_freq = [(word, sum_words[0, idx]) \n",
    "                      for word, idx in vec.vocabulary_.items()]\n",
    "        words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n",
    "        return words_freq[:10]\n",
    "top_n_bigrams=_get_top_ngram(text,n)[:10]\n",
    "    x,y=map(list,zip(*top_n_bigrams))\n",
    "    sns.barplot(x=y,y=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis, ovde zapravo pravimo prediction odnosa na osnovu tokena\n",
    "\n",
    "#ovo prvo samo plotuje koliko ima \"negativnih\" a koliko \"pozitivnih\" interakcija, mi treba da imamo vise klasa od te dve\n",
    "from textblob import TextBlob\n",
    "    \n",
    "def plot_polarity_histogram(text):\n",
    "    \n",
    "    def polarity(text):\n",
    "        return TextBlob(text).sentiment.polarity\n",
    "    \n",
    "    df_x = pd.DataFrame() \n",
    "    polarity_score =text.apply(lambda x : polarity(x))\n",
    "    df_filtered['polarity_score']=df_filtered['text'].\\\n",
    "       apply(lambda x : polarity(x))\n",
    "    polarity_score.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ovo je slicno, imamo positive, negative i neutral klase\n",
    "\n",
    "from textblob import TextBlob\n",
    "import matplotlib.pyplot as plt\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "def sentiment_vader(text, sid):\n",
    "    ss = sid.polarity_scores(text)\n",
    "    ss.pop('compound')\n",
    "    return max(ss, key=ss.get)\n",
    "def sentiment_textblob(text):\n",
    "        x = TextBlob(text).sentiment.polarity\n",
    "        \n",
    "        if x<0:\n",
    "            return 'Negative'\n",
    "        elif x==0:\n",
    "            return 'Neutral'\n",
    "        else:\n",
    "            return 'Positive'\n",
    "        \n",
    "def sentiment_x(x):\n",
    "    if x<0:\n",
    "        return 'Negative'\n",
    "    elif x==0:\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return 'Positive' \n",
    "        \n",
    "def plot_sentiment_barchart(text, method):\n",
    "    if method == 'TextBlob':\n",
    "        sentiment = text.map(lambda x: sentiment_textblob(x))\n",
    "    elif method == 'Vader':\n",
    "        nltk.download('vader_lexicon')\n",
    "        sid = SentimentIntensityAnalyzer()\n",
    "        sentiment = text.map(lambda x: sentiment_vader(x, sid=sid))\n",
    "    else:\n",
    "        raise ValueError('Textblob or Vader')\n",
    "    \n",
    "    df_filtered['polarity']=df_filtered['polarity_score'].\\\n",
    "       apply(lambda x : sentiment_x(x))\n",
    "  \n",
    "    plt.figure(figsize=(5, 2), dpi=100)\n",
    "    plt.bar(sentiment.value_counts().index,\n",
    "            sentiment.value_counts())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
